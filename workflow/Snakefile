
from ConfigParser import ConfigParser

configfile: './config/config.yaml'

parser = ConfigParser(config)
final_files = parser.file_names["pipeline_output"] 


# Ensures dataset and method are names not file paths, and ids are ints
wildcard_constraints:
    dataset="[^\/]+",
    eval_dataset="[^\/]+",
    method="[^\/]+",
    metric="[^\/]+",
    data_id="\d+",
    eval_data_id="\d+",
    selection_id="\d+"
    


rule all:
    input:
        expand("{results_dir}/{files}", results_dir=config['RESULTS_DIR'], files=final_files)
        

rule process_data:
    threads: 12
    resources:
        mem_mb = lambda wildcards, attempt: 64000 * (attempt + int(attempt > 1))
    conda:
        "envs/base_env.yaml"
    input:
        expand("{raw_data_dir}/{{dataset}}.h5ad", raw_data_dir=config['DATA_DIR'])
    output:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
        #temp(expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP']))
    params:
        data_params_file = parser.data_params_file
    shell:
        "python workflow/scripts/process_data.py --data {input} -o {output} -id {wildcards.data_id} -f {params.data_params_file}"


#rule selection_py: # spapros, asfs, cosg, nsforest, scmer, triku, smash
#    input:
#        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
#    output:
#        expand("{results_dir}/selection/{{method}}_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
#    conda:
#        "envs/venv_{method}.yaml"
#    log:
#        "logs/selections/{method}_{selection_id}_{dataset}_{data_id}.csv"
#    params:
#        selection_params_file = parser.selection_params_file
#    shell:
#        "python workflow/scripts/selection.py --data {input} -o {output} --method {wildcards.method}  -id {wildcards.selection_id} -f {params.selection_params_file}"
        
rule selection_spapros: 
    threads: 12
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/spapros_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/spapros_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_spapros.yaml"
    log:
        "logs/selections/spapros_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method spapros  "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_spaproscto: 
    threads: 12
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/spaproscto_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/spaproscto_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_spapros.yaml"
    log:
        "logs/selections/spaproscto_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method spaproscto  "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_DE: 
    threads: 12
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/DE_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/DE_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_spapros.yaml"
    log:
        "logs/selections/DE_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method DE "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_pca:
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/pca_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/pca_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_spapros.yaml"
    threads: 12        
    log:
        "logs/selections/pca_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method pca "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"

rule selection_persist:
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/persist_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/persist_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_persist.yaml"
    threads: 6
    resources:
        partition="gpu_p",
        qos="gpu_normal",
        mem_mb=64000,
        gpu=1
        #gres="gpu:1" 
    log:
        "logs/selections/persist_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method persist "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_persistus: # persist with unsupervised optimisation
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/persistus_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/persistus_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_persist.yaml"
    threads: 6
    resources:
        partition="gpu_p",
        qos="gpu_normal",
        mem_mb=64000,
        gpu=1
        #gres="gpu:1"        
    log:
        "logs/selections/persistus_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method persistus "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"

rule selection_scgenefit: # spapros, asfs, cosg, nsforest, scmer, triku, smash
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/scgenefit_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/scgenefit_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_scgenefit.yaml"
    threads: 12
    log:
        "logs/selections/scgenefit_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method scgenefit "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_nsforest: 
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/nsforest_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    conda:
        "envs/venv_selection.yaml"
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/nsforest_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    threads: 12        
    log:
        "logs/selections/nsforest_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method nsforest "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_scmer: # spapros, asfs, cosg, nsforest, scmer, triku, smash
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/scmer_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/scmer_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_selection.yaml"
    threads: 12        
    log:
        "logs/selections/scmer_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method scmer "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"

rule selection_smash: 
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/smash_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/smash_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_smash.yaml"
    threads: 12
    resources:
        mem_mb=160000
    log:
        "logs/selections/smash_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method smash "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
      
rule selection_asfs: 
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/asfs_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/asfs_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_asfs.yaml"
    threads: 12        
    log:
        "logs/selections/asfs_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method asfs "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
      
rule selection_cosg:
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/cosg_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/cosg_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_cosg.yaml"
    threads: 12        
    log:
        "logs/selections/cosg_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method cosg "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"

rule selection_triku:
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/triku_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/triku_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    conda:
        "envs/venv_triku.yaml"
    threads: 12        
    log:
        "logs/selections/triku_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method triku "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
      
rule selection_selfe: 
    container:
        "docker://louisk92/spapros_selfe:v1.0"
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/selfe_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/selfe_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    threads: 12        
    log:
        "logs/selections/selfe_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method selfe "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"

rule selection_genebasis:
    container:
        "docker://louisk92/spapros_genebasis:v1.0"
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/genebasis_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/genebasis_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    threads: 12
    resources:
        mem_mb=160000
    log:
        "logs/selections/genebasis_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method genebasis "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
rule selection_scpnmf:
    container:
        "docker://louisk92/spapros_scpnmf:v1.0"
    input:
        expand("{data_dir}/{{dataset}}_{{data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output:
        expand("{results_dir}/selection/scpnmf_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/selection/scpnmf_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    threads: 12
    resources:
        mem_mb=160000
    log:
        "logs/selections/scpnmf_{selection_id}_{dataset}_{data_id}.txt"
    params:
        selection_params_file = parser.selection_params_file,
        save_method_specific_output = config['SAVE_METHOD_SPECIFIC_OUTPUT']
    shell:
        "python workflow/scripts/selection.py "
        "--data {input} "
        "-o {output} "
        "--method scpnmf "
        "-id {wildcards.selection_id} "
        "-f {params.selection_params_file} "
        "-s {params.save_method_specific_output}"
        
        
# ----------------------------------------------------------
# Evaluation
# ----------------------------------------------------------

rule eval0_shared:
    input:
        data_path = expand("{data_dir}/{{eval_dataset}}_{{eval_data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output: 
        expand("{results_dir}/evaluation/references/{{eval_dataset}}_{{eval_data_id}}_{{metric}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/evaluation/eval0_{{eval_dataset}}_{{eval_data_id}}_{{metric}}.benchmark.txt"
    threads: 12    
    conda:
        "envs/venv_spapros.yaml"    
    params:
        data_path = config['DATA_DIR_TMP'] + "/{eval_dataset}_{eval_data_id}.h5ad",
        results_dir = config['RESULTS_DIR'],
        dataset_params_file = parser.data_params_file
    shell:
        "python workflow/scripts/evaluation.py "
        "--eval_step shared "
        "--metric {wildcards.metric} "
        "--data {input.data_path} "
        "--results_dir {params.results_dir} "
        "--dataset_params_file {params.dataset_params_file} "

rule eval1_pre:
    input: 
        expand("{results_dir}/selection/{{method}}_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR']),
        data_path = expand("{data_dir}/{{eval_dataset}}_{{eval_data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output: 
        expand("{results_dir}/evaluation/{{metric}}/{{metric}}_{{eval_dataset}}_{{eval_data_id}}_{{method}}_{{selection_id}}_{{dataset}}_{{data_id}}_pre.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/evaluation/eval1_{{metric}}_{{eval_dataset}}_{{eval_data_id}}_{{method}}_{{selection_id}}_{{dataset}}_{{data_id}}_pre.benchmark.txt"
    threads: 12    
    conda:
        "envs/venv_spapros.yaml"    
    params:
        results_dir = config['RESULTS_DIR'],
        selection_name = "{method}_{selection_id}_{dataset}_{data_id}",
        evaluation_overview_file = parser.evaluation_overview_file,
        dataset_params_file = parser.data_params_file
    shell:
        "python workflow/scripts/evaluation.py "
        "--eval_step pre "
        "--metric {wildcards.metric} "
        "--data {input.data_path} "
        "--results_dir {params.results_dir} "
        "--selection {params.selection_name} "
        "--dataset_params_file {params.dataset_params_file} "


def get_eval_main_inputs(wildcards, results_dir=config['RESULTS_DIR'], data_dir=config['DATA_DIR_TMP']):
    """List up input files for the main evaluation step rule for a given metric
    
    Some metrics don't have "shared" or "pre" results, therefore we need to create a filtered list of input files.
    """
    data_path = f"{data_dir}/{wildcards.eval_dataset}_{wildcards.eval_data_id}.h5ad"
    eval_dataset = f"{wildcards.eval_dataset}_{wildcards.eval_data_id}"
    selection = f"{wildcards.method}_{wildcards.selection_id}_{wildcards.dataset}_{wildcards.data_id}"

    probe_set_file = f"{results_dir}/selection/{selection}.csv"
    shared = f"{results_dir}/evaluation/references/{eval_dataset}_{wildcards.metric}.csv"
    pre_file_name = f"{results_dir}/evaluation/{wildcards.metric}/{wildcards.metric}_{eval_dataset}_{selection}_pre.csv"
    
    # Some metrics don't have "shared" or "pre" results, add them conditionally
    input_files = {
        "data_path": data_path, 
        "probe_set_file" : probe_set_file
    }
    if wildcards.metric != "forest_clfs":
        input_files["shared"] = shared
    if wildcards.metric not in ["forest_clfs", "marker_corr", "gene_corr"]:
        input_files["pre"] = pre_file_name
    
    return input_files

rule eval2_main:
    input: 
        unpack(get_eval_main_inputs)
    output:
        expand("{results_dir}/evaluation/{{metric}}/{{metric}}_{{eval_dataset}}_{{eval_data_id}}_{{method}}_{{selection_id}}_{{dataset}}_{{data_id}}.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/evaluation/eval2_{{metric}}_{{eval_dataset}}_{{eval_data_id}}_{{method}}_{{selection_id}}_{{dataset}}_{{data_id}}.benchmark.txt"
    threads: 12    
    conda:
        "envs/venv_spapros.yaml"    
    params:
        results_dir = config['RESULTS_DIR'],
        selection_name = "{method}_{selection_id}_{dataset}_{data_id}",
        evaluation_overview_file = parser.evaluation_overview_file,
        dataset_params_file = parser.data_params_file
    shell:
        "python workflow/scripts/evaluation.py "
        "--eval_step main "
        "--metric {wildcards.metric} "
        "--data {input.data_path} "
        "--results_dir {params.results_dir} "
        "--selection {params.selection_name} "
        "--dataset_params_file {params.dataset_params_file} "     


def get_evaluation_files_to_summarise(wildcards, results_dir=config['RESULTS_DIR']):
    """List up all evaluation files that are summarised in one table (i.e. all evaluations of a given evaluation dataset)
    """
    files = parser.get_evaluation_files_to_summarise(wildcards.eval_dataset, wildcards.eval_data_id)
    return [f"{results_dir}/{f}" for f in files]

rule eval3_summary:
    input:
        get_evaluation_files_to_summarise,
        data_path = expand("{data_dir}/{{eval_dataset}}_{{eval_data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output: 
        expand("{results_dir}/evaluation/{{eval_dataset}}_{{eval_data_id}}_summary.csv", results_dir=config['RESULTS_DIR'])
    benchmark:
        f"{config['RESULTS_DIR']}/benchmarks/evaluation/eval3_{{eval_dataset}}_{{eval_data_id}}_summary.benchmark.txt"
    threads: 12    
    conda:
        "envs/venv_spapros.yaml"    
    params:
        results_dir = config['RESULTS_DIR'],
        evaluation_overview_file = parser.evaluation_overview_file,
        dataset_params_file = parser.data_params_file
    shell:
        "python workflow/scripts/evaluation.py "
        "--eval_step summary "
        "--data {input.data_path} "
        "--results_dir {params.results_dir} "
        "--evaluation_overview_file {params.evaluation_overview_file} " 
        "--dataset_params_file {params.dataset_params_file} "     


rule special_prelim_eval3_summary:
    input:
        get_evaluation_files_to_summarise,
        data_path = expand("{data_dir}/{{eval_dataset}}_{{eval_data_id}}.h5ad", data_dir=config['DATA_DIR_TMP'])
    output: 
        expand("{results_dir}/evaluation/prelim_summary/{{eval_dataset}}_{{eval_data_id}}_summary.csv", results_dir=config['RESULTS_DIR'])
    threads: 12    
    conda:
        "envs/venv_spapros.yaml"    
    params:
        results_dir = config['RESULTS_DIR'],
        evaluation_overview_file = parser.evaluation_overview_file,
        dataset_params_file = parser.data_params_file
    shell:
        "python workflow/scripts/evaluation.py "
        "--eval_step summary "
        "--data {input.data_path} "
        "--results_dir {params.results_dir} "
        "--evaluation_overview_file {params.evaluation_overview_file} " 
        "--dataset_params_file {params.dataset_params_file} " 
        "--prelim_summary True " 


